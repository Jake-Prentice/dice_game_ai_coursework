**The approach:**
In this dice game, given some initial roll of the dice, the agent has to decide whether to stick with the current state of the dice or re-roll any number of them. This game involves making decisions that involve uncertainty as it’s not immediately obvious what dice to re-roll given a current state to maximise the score. This makes it suitable to model as a Markov Decision Process (MDP). Given the probability of getting a specific dice roll can be calculated given the action taken, and the reward is also known, other reinforcement methods would be overkill. Therefore, I chose to experiment between two algorithms, value iteration and policy iteration.<br>
Value iteration is a dynamic programming algorithm to find the optimal value function for a MDP. The optimal value function gives the maximum expected reward that can be obtained from any state by following the optimal policy function.<br>
I managed to speed up the algorithm significantly by caching the outputs of the ‘get_next_state’ method since the output will always be the same given the same action and state. The operations within that method are also quite computationally expensive which adds up over so many iterations.<br>
By experimenting with various theta and gamma values I was able to find the optimal combination of the two for algorithmic speed and performance.
At the default gamma of 0.9, by increasing theta, the performance of the value iteration algorithm would increase significantly. However, the overall average score would decrease. So I settled for a value of 0.5 as a compromise for efficiency and performance.<br>
For the gamma value it was more straightforward, although increasing gamma increases the time, the significant improvement in the average score makes it worth it. So, I settled for a value of 0.96.<br>
Policy iteration is the other algorithm I experimented with. Instead of finding an optimal value function and then finding the optimal policy from that, policy iteration begins with a random policy, finds the value function of that policy and then improves that policy greedily from the current value function. Policy iteration converges faster than value iteration however each iteration is much more computationally expensive that that of value iteration.<br>
My expectation was that policy iteration would perform much faster, but it turned out that value iteration would perform consistently, and significantly better. Maybe I just coded a particularly inefficient version of it. however, whereas value iteration would take between 0.1 and 0.2 seconds for 3 regular dice, policy iteration would take between 1 and 2 seconds.
<br>Regarding the selection of theta and gamma values, I found similar results to that of value iteration, so I settled for the same values.
